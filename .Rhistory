melt() %>%
na.omit() %>%
data.table()
ggplot(df.melt,aes(value))+
geom_density(fill="red",alpha=.3,col="blue")+
facet_wrap(~variable,scales="free")
rm(df.melt)
g1 <- ggplot(df,aes(season))+
geom_bar(stat="count",fill="blue",alpha=.3,col="red")
g2 <- ggplot(df,aes(size))+
geom_bar(stat="count",fill="blue",alpha=.3,col="red")
g3 <- ggplot(df,aes(speed))+
geom_bar(stat="count",fill="blue",alpha=.3,col="red")
multiplot(g1,g2,g3,cols=3)
rm(multiplot,g1,g2,g3)
cor.data <- df %>%
select(-season,-size,-speed) %>%
cor(use="pairwise.complete.obs") %>%
as.matrix() %>%
data.table()
diag(cor.data) <- NA
ggcorrplot(cor.data)
rm(cor.data)
md.pattern(df,rotate.names=T)
missing.data <- data.table(Chla.missing=is.na(df$Chla),
Cl.missing=is.na(df$Cl),
PO4.missing=is.na(df$PO4),
mxPH.missing=is.na(df$mxPH),
mnO2.missing=is.na(df$mnO2),
NO3.missing=is.na(df$NO3),
NH4.missing=is.na(df$NH4),
oPO4.missing=is.na(df$oPO4))
df.complete <- df %>%
select(-season,-size,-speed) %>%
mice(method = "rf", m = 1, maxit = 1) %>%
complete() %>%
cbind(missing.data) %>%
mutate_if(is.logical,as.numeric) %>%
data.table()
rm(missing.data)
# Remove a1 and a2 to prevent leakage
df.cluster <- df.complete %>%
select(-a1,-a2) %>%
scale(center=T,scale=T) %>%
data.table()
set.seed(123)
wss <- (nrow(df.cluster)-1)*sum(apply(df.cluster,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(df.cluster,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
# Conclusion: We'll go with a 7 cluster kmeans
fit <- kmeans(df.cluster, 7)
df.data <- data.table(df.complete,kmeans=fit$cluster) %>%
mutate(value=1) %>%
mutate(kmeans=paste("kmeans",kmeans,sep='')) %>%
spread(kmeans,value,fill=0) %>%
data.table()
rm(i,wss,fit)
df.data[,a1a2:=as.factor(ifelse((a1>15|a2>15),"ok","toxic"))]
df.data2 <- df.data %>%
select(-a1,-a2) %>%
data.table()
set.seed(123)
train_ind <- createDataPartition(df.data2$a1a2,p=0.8,list=F)
valid <- df.data2[-train_ind,]
train <- df.data2[train_ind,]
set.seed(123)
test_ind <- createDataPartition(train$a1a2,p=0.2,list=F)
train <- train[-test_ind,]
test <- train[test_ind]
rm(train_ind)
# mmc function borrowed from package mccr
mcc <- function (act, pred){
TP <- sum(act == 1 & pred == 1)
TN <- sum(act == 0 & pred == 0)
FP <- sum(act == 0 & pred == 1)
FN <- sum(act == 1 & pred == 0)
denom <- as.double(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)
if (any((TP+FP) == 0, (TP+FN) == 0, (TN+FP) == 0, (TN+FN) == 0)) denom <- 1
mcc <- ((TP*TN)-(FP*FN)) / sqrt(denom)
return(mcc)
}
model.c5 <- C5.0(x = as.matrix(select(train,-a1a2)), y = train$a1a2)
mcc.c5 <- mcc(test$a1a2,predict(model.c5,newdata=test,type="class"))
summary(test$a1a2)
summary(predict(model.c5,newdata=test,type="class"))
summary(df$a1)
summary(df$a2)
summary(df.data$a1a2)
# Built under R version 3.5.3
rm(list=ls())
library(data.table)
library(tidyverse)
library(ggcorrplot)
library(mice)
library(caret)
library(C50)
library(naivebayes)
library(glmnet)
df <- fread("algae_blooms.csv")
source("./functions/multiplot.R")
df <- fread("algae_blooms.csv")
# One hot encoding will be done in a way to preserve the original categorical variables
season <- df$season
size <- df$size
speed <- df$speed
df <- df %>%
mutate(value=1) %>%
spread(season,value,fill=0) %>%
mutate(season=season) %>%
mutate(value=1) %>%
spread(size,value,fill=0) %>%
mutate(size=size) %>%
mutate(value=1) %>%
spread(speed,value,fill=0) %>%
mutate(speed=speed) %>%
data.table()
setcolorder(df,c("season","size","speed"))
rm(season,size,speed)
df.melt <- df %>%
select(-season,-size,-speed) %>%
melt() %>%
na.omit() %>%
data.table()
ggplot(df.melt,aes(value))+
geom_density(fill="red",alpha=.3,col="blue")+
facet_wrap(~variable,scales="free")
rm(df.melt)
g1 <- ggplot(df,aes(season))+
geom_bar(stat="count",fill="blue",alpha=.3,col="red")
g2 <- ggplot(df,aes(size))+
geom_bar(stat="count",fill="blue",alpha=.3,col="red")
g3 <- ggplot(df,aes(speed))+
geom_bar(stat="count",fill="blue",alpha=.3,col="red")
multiplot(g1,g2,g3,cols=3)
rm(multiplot,g1,g2,g3)
cor.data <- df %>%
select(-season,-size,-speed) %>%
cor(use="pairwise.complete.obs") %>%
as.matrix() %>%
data.table()
diag(cor.data) <- NA
ggcorrplot(cor.data)
rm(cor.data)
md.pattern(df,rotate.names=T)
missing.data <- data.table(Chla.missing=is.na(df$Chla),
Cl.missing=is.na(df$Cl),
PO4.missing=is.na(df$PO4),
mxPH.missing=is.na(df$mxPH),
mnO2.missing=is.na(df$mnO2),
NO3.missing=is.na(df$NO3),
NH4.missing=is.na(df$NH4),
oPO4.missing=is.na(df$oPO4))
df.complete <- df %>%
select(-season,-size,-speed) %>%
mice(method = "rf", m = 1, maxit = 1) %>%
complete() %>%
cbind(missing.data) %>%
mutate_if(is.logical,as.numeric) %>%
data.table()
rm(missing.data)
# Remove a1 and a2 to prevent leakage
df.cluster <- df.complete %>%
select(-a1,-a2) %>%
scale(center=T,scale=T) %>%
data.table()
set.seed(123)
wss <- (nrow(df.cluster)-1)*sum(apply(df.cluster,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(df.cluster,centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
# Conclusion: We'll go with a 7 cluster kmeans
fit <- kmeans(df.cluster, 7)
df.data <- data.table(df.complete,kmeans=fit$cluster) %>%
mutate(value=1) %>%
mutate(kmeans=paste("kmeans",kmeans,sep='')) %>%
spread(kmeans,value,fill=0) %>%
data.table()
rm(i,wss,fit)
set.seed(123)
train_ind <- createDataPartition(df.data2$a1a2,p=0.8,list=F)
df.data2[,a1a2:=as.factor(ifelse((a1>15|a2>15),"ok","toxic"))]
df.data[,a1a2:=as.factor(ifelse((a1>15|a2>15),"ok","toxic"))]
df.data2 <- df.data %>%
select(-a1,-a2) %>%
data.table()
set.seed(123)
train_ind <- createDataPartition(df.data2$a1a2,p=0.8,list=F)
valid <- df.data2[-train_ind,]
train <- df.data2[train_ind,]
set.seed(123)
test_ind <- createDataPartition(train$a1a2,p=0.2,list=F)
train <- train[-test_ind,]
test <- train[test_ind]
rm(train_ind)
summary(df.data2$a1a2)
summary(test$a1a2)
set.seed(123)
train_ind <- sample(seq_len(nrow(df.data2)), size = floor(0.8 * nrow(df.data2)))
valid <- df.data2[-train_ind,]
train <- df.data2[train_ind,]
set.seed(123)
test_ind <- sample(seq_len(nrow(df.data2)), size = floor(0.2 * nrow(df.data2)))
train <- train[-test_ind,]
test <- train[test_ind]
rm(train_ind)
summary(test$a1a2)
summary(test$a1a2)
set.seed(123)
train_ind <- sample(seq_len(nrow(df.data2)), size = floor(0.8 * nrow(df.data2)))
valid <- df.data2[-train_ind,]
train <- df.data2[train_ind,]
set.seed(123)
test_ind <- sample(seq_len(nrow(train)), size = floor(0.2 * nrow(train)))
train <- train[-test_ind,]
test <- train[test_ind]
rm(train_ind)
summary(test$a1a2)
df.data[,a1a2:=as.factor(ifelse((a1>15|a2>15),"ok","toxic"))]
df.data2 <- df.data %>%
select(-a1,-a2) %>%
copy() %>%
data.table()
set.seed(123)
train_ind <- sample(seq_len(nrow(df.data2)), size = floor(0.8 * nrow(df.data2)))
valid <- df.data2[-train_ind,]
train <- df.data2[train_ind,]
set.seed(123)
test_ind <- sample(seq_len(nrow(train)), size = floor(0.2 * nrow(train)))
train <- train[-test_ind,]
test <- train[test_ind]
rm(train_ind)
summary(test$a1a2)
View(test)
View(valid)
sum(is.na(df.data2))
sum(is.na(train))
sum(is.na(valid))
set.seed(123)
train_ind <- sample(seq_len(nrow(df.data2)), size = floor(0.8 * nrow(df.data2)))
valid <- df.data2[-train_ind,]
train <- df.data2[train_ind,]
set.seed(123)
test_ind <- sample(seq_len(nrow(train)), size = floor(0.2 * nrow(train)))
test <- train[test_ind]
train <- train[-test_ind,]
rm(train_ind)
summary(test$a1a2)
mcc <- function (act, pred){
TP <- sum(act == 1 & pred == 1)
TN <- sum(act == 0 & pred == 0)
FP <- sum(act == 0 & pred == 1)
FN <- sum(act == 1 & pred == 0)
denom <- as.double(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)
if (any((TP+FP) == 0, (TP+FN) == 0, (TN+FP) == 0, (TN+FN) == 0)) denom <- 1
mcc <- ((TP*TN)-(FP*FN)) / sqrt(denom)
return(mcc)
}
rm(train_ind,test_ind)
model.c5 <- C5.0(x = as.matrix(select(train,-a1a2)), y = train$a1a2)
summary(test$a1a2)
summary(predict(model.c5,newdata=test,type="class"))
mcc.c5 <- mcc(test$a1a2,predict(model.c5,newdata=test,type="class"))
table(test$a1a2,predict(model.c5,newdata=test,type="class"))
mcc.c5 <- mcc(test$a1a2=="ok",predict(model.c5,newdata=test,type="class")=="ok")
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
prob <- i /100
results$threshold[i] <- prob
predictions <- predict(model.c5,newdata=test,type="prob")[,1]>prob
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1){
next
}
results$mmc[i] <- mmc(actuals,predictions)
}
# Test different thresholds to improve the MMC metric
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
prob <- i /100
results$threshold[i] <- prob
predictions <- predict(model.c5,newdata=test,type="prob")[,1]>prob
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1){
next
}
results$mmc[i] <- mcc(actuals,predictions)
}
View(results)
model.c5 <- C5.0(x = as.matrix(select(train,-a1a2)), y = train$a1a2)
summary(test$a1a2)
summary(predict(model.c5,newdata=test,type="class"))
mcc.c5 <- mcc(test$a1a2=="ok",predict(model.c5,newdata=test,type="class")=="ok")
# Test different thresholds to improve the MMC metric -------------------------
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
prob <- i /100
results$threshold[i] <- prob
predictions <- predict(model.c5,newdata=test,type="prob")[,1]>prob
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1){
next
}
results$mmc[i] <- mcc(actuals,predictions)
}
# Plot the results ------------------------------------------------------------
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = cm.c5$mmc,size=2,alpha=0.2,col="blue")+
labs(title="Performance of C5 Algorithm",
x="Threshold",
y="Matthews correlation coefficient")
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = mmc.c5,size=2,alpha=0.2,col="blue")+
labs(title="Performance of C5 Algorithm",
x="Threshold",
y="Matthews correlation coefficient")
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = mcc.c5,size=2,alpha=0.2,col="blue")+
labs(title="Performance of C5 Algorithm",
x="Threshold",
y="Matthews correlation coefficient")
mcc.c5.opt <- max(results$mmc)
model.nb <- naive_bayes(x = as.matrix(select(train,-a1a2)), y = train$a1a2)
mcc.nb <- mcc(test$a1a2=="ok",predict(model.nb,newdata=test,type="class")=="ok")
model.nb <- naive_bayes(x = as.matrix(select(train,-a1a2)), y = train$a1a2,usekernel = T)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.nb <- mcc(test$a1a2=="ok",predict(model.nb,newdata=test,type="class")=="ok")
# Test different thresholds to improve the MMC metric -------------------------
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
results$threshold[i] <- i /100
predictions <- predict(model.nb,newdata=test,type="prob")[,1]>(i /100)
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1)(next)
results$mmc[i] <- mcc(actuals,predictions)
}
mcc.nb.opt <- max(results$mmc)
# Plot the results ------------------------------------------------------------
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = mcc.c5,size=2,alpha=0.2,col="blue")+
labs(title="Performance",
x="Threshold",
y="Matthews correlation coefficient")
model.glm <- cv.glmnet(x = as.matrix(select(train,-a1a2)), y = train$a1a2,family="binomial")
mcc.nb <- mcc(test$a1a2=="ok",predict(model.glm,newx=data.matrix(test),type="class")=="ok")
mcc.nb <- mcc(test$a1a2=="ok",predict(model.glm,newx=data.matrix(select(test,-a1a2),type="class")=="ok")
)
# Train the model -------------------------------------------------------------
model.glmnet <- cv.glmnet(x = data.matrix(select(train,-a1a2)), y = train$a1a2, family = "binomial", type.measure = "auc",nfolds=5)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.glmnet <- mcc(test$a1a2=="ok",predict(model.,newdata=data.matrix(select(test,-a1a2)),type="class",s = "lambda.1se")=="ok")
mcc.glmnet <- mcc(test$a1a2=="ok",predict(model.glmnet,newdata=data.matrix(select(test,-a1a2)),type="class",s = "lambda.1se")=="ok")
mcc.glmnet <- mcc(test$a1a2=="ok",predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="class",s = "lambda.1se")=="ok")
# Test different thresholds to improve the MMC metric -------------------------
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
results$threshold[i] <- i /100
predictions <- predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="response",s = "lambda.1se")>(i /100)
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1)(next)
results$mmc[i] <- mcc(actuals,predictions)
}
mcc.nb.opt <- max(results$mmc)
# Plot the results ------------------------------------------------------------
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = mcc.c5,size=2,alpha=0.2,col="blue")+
labs(title="Performance",
x="Threshold",
y="Matthews correlation coefficient")
# Train the model -------------------------------------------------------------
model.glmnet <- cv.glmnet(x = data.matrix(select(train,-a1a2)), y = train$a1a2, family = "binomial", type.measure = "auc",nfolds=5)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.glmnet <- mcc(test$a1a2=="ok",predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="class",s = "lambda.1se")=="ok")
# Test different thresholds to improve the MMC metric -------------------------
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
results$threshold[i] <- i /100
predictions <- predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="response",s="lambda.1se")>(i /100)
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1)(next)
results$mmc[i] <- mcc(actuals,predictions)
}
mcc.glmnet.opt <- max(results$mmc)
# Plot the results ------------------------------------------------------------
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = mcc.c5,size=2,alpha=0.2,col="blue")+
labs(title="Performance",
x="Threshold",
y="Matthews correlation coefficient")
summary(predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="response",s="lambda.1se"))
length(predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="response",s="lambda.1se"))
length(predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="class",s="lambda.1se"))
x <- data.table(x=predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="class",s="lambda.1se"),predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="response",s="lambda.1se")
)
View(x)
# Train the model -------------------------------------------------------------
model.glmnet <- cv.glmnet(x = data.matrix(select(train,-a1a2)), y = train$a1a2, family = "binomial", type.measure = "auc",nfolds=5)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.glmnet <- mcc(test$a1a2=="ok",predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="class",s = "lambda.1se")=="ok")
# Test different thresholds to improve the MMC metric -------------------------
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
results$threshold[i] <- i /100
predictions <- predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="response",s="lambda.1se")<(i /100)
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1)(next)
results$mmc[i] <- mcc(actuals,predictions)
}
mcc.glmnet.opt <- max(results$mmc)
# Plot the results ------------------------------------------------------------
ggplot(results,aes(threshold,mmc))+
geom_point()+geom_line()+
geom_vline(xintercept = 0.5,size=2,alpha=0.2,col="blue")+
geom_hline(yintercept = mcc.c5,size=2,alpha=0.2,col="blue")+
labs(title="Performance",
x="Threshold",
y="Matthews correlation coefficient")
mcc.glmnet <- mcc(test$a1a2=="ok",predict(model.glmnet,newx=data.matrix(select(test,-a1a2)),type="class",s = "lambda.min")=="ok")
library(e1071)
help(svm)
model.svm = svm(a1a2 ~ ., data = train, kernel = "linear", type="C", cost = 10, scale = FALSE)
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newx=data.matrix(select(test,-a1a2)),type="class",s = "lambda.1se")=="ok")
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
model.svm = svm(a1a2 ~ ., data = train, kernel = "linear", type="C", cost = 10, scale = T)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
model.svm = svm(a1a2 ~-mnO2.missing ., data = train, kernel = "linear", type="C", cost = 10, scale = T)
model.svm = svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=10,scale=T)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
model.svm = svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=10,scale=F)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
results <- data.table(iter=numeric(20),
mcc=numeric(20))
for (i in 1:20){
# Train the model -------------------------------------------------------------
model.svm = svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=10,scale=F)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
results$iter[i] <- i
results$mcc[i] <- mcc.svm
}
View(results)
results <- data.table(iter=numeric(20),
mcc=numeric(20))
for (i in 1:20){
# Train the model -------------------------------------------------------------
model.svm = svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=i,scale=F)
# Calculate Matthews correlation coefficient ----------------------------------
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
results$iter[i] <- i
results$mcc[i] <- mcc.svm
}
View(restuls
View(results)
ggplot(results,aes(iter,mcc))+
geom_point()geom_line()
ggplot(results,aes(iter,mcc))+
geom_point()+geom_line()
ggplot(results,aes(iter,mcc))+
geom_point()+geom_smooth(method='loess')
results[which.max(cost)]$iter
results[cost==which.max(cost)]$iter
results[mcc==which.max(mcc)]$iter
results[mcc==pmax(mcc)]$iter
results[mcc==which.max(mcc)]$iter
results[which.max(mcc)]$iter
mcc.svm <- results[which.max(mcc)]$mcc
results[which.max(mcc)]$iter
svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F)
model.svm <- svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F)
help("predict.svm")
model.svm <- svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F)
results <- data.table(threshold=numeric(100),mmc=numeric(100))
for (i in 1:100){
results$threshold[i] <- i /100
predictions <- predict(model.svm,
newdata=test,
probability=T) >(i/100)
actuals <- as.logical(test$a1a2=="ok")
if (length(unique(predictions))==1)(next)
results$mmc[i] <- mcc(actuals,predictions)
}
mcc.svm.opt <- max(results$mmc)
model.svm <- svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F,probability=T)
help(svm)
model.svm <- svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F,probability=T)
model.svm <- svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F,probability=1)
model.svm <- svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=results[which.max(mcc)]$iter,scale=F)
# Try out different values for the cost parameter -----------------------------
results <- data.table(iter=numeric(20),
mcc=numeric(20))
for (i in 1:20){
model.svm = svm(a1a2 ~.-mnO2.missing,data=train,kernel="linear",type="C",cost=i,scale=F)
mcc.svm <- mcc(test$a1a2=="ok",predict(model.svm,newdata=test,type="class")=="ok")
results$iter[i] <- i
results$mcc[i] <- mcc.svm
}
# Plot the results of tuning the cost parameter -------------------------------
ggplot(results,aes(iter,mcc))+
geom_point()+geom_smooth(method='loess')
